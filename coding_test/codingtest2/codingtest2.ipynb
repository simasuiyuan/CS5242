{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS5242 : Neural Networks and Deep Learning\n",
    "##Â Sem 1, 2021\n",
    "### Lecturer: Xavier Bresson\n",
    "### TAs: Goh Yong Liang, Wang Guangzhi, Hu Sixu, Wu Zhaomin, Liu Hongfu, Fu Yujian, Liu Xu\n",
    "\n",
    "\n",
    "\n",
    "## Coding Test 2\n",
    "Date: November 2nd, 2021<br> \n",
    "Time: 8pm-9:30 (90min)<br>\n",
    "\n",
    "*Instructions* <br>\n",
    "Name: Do not forget to add your name to the notebook file \"codingtest2_NAME_STUDENT_ID.ipynb\". Penalty: You lose 50% of the grade if the notebook cannot be identified with your name.<br>\n",
    "Timestamps: You lose 50% of the grade if any time stamp of the notebook is beyond 9:30pm.<br>\n",
    "LumiNUS submission: You will get grade zero if you do not upload the notebook by 9:35pm.<br>\n",
    "Questions: This notebook has 10 questions.<br>\n",
    "Answers: Write the answers to each question in this notebook.<br>\n",
    "Grading: 1 point for each question.<br>\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Given an input image of size (1, 3, 128, 128) in the form of (batch size, number of channels, height, width) which is passed to a 2D convolutional layer composed of 32 filters with filter size (3,3) in the form of (height, width), no padding and stride value 5. <br>\n",
    "Code this 2D convolutional layer and print the size of the output image. <br>\n",
    "\n",
    "Hint: You may use function `torch.nn.Conv2d`.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--20-02-13\n",
      "torch.Size([1, 32, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "x = torch.rand(1, 3, 128, 128) # input image\n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "import torch.nn as nn\n",
    "mod = nn.Conv2d( 3 , 32 ,  kernel_size=3,  padding=0, stride= 5)\n",
    "\n",
    "y = mod(x)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Given an image of size (100, 3, 32, 32) in the form of (batch size, number of channels, height, width) which is passed to a 2D max pooling layer to reduce the size of the input image to (100, 3, 8, 8). <br>\n",
    "Code this 2D max pooling layer and print the size of the output image. <br>\n",
    "\n",
    "Hint: You may use function `torch.nn.MaxPool2d`.<br>\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--20-03-09\n",
      "torch.Size([100, 3, 8, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "x = torch.rand(100, 3, 32, 32) # input image\n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "import torch.nn as nn\n",
    "mod = nn.MaxPool2d(4,4)\n",
    "y=mod(x)\n",
    "\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Given an image of size (512, 3, 64, 64) in the form of (batch size, number of channels, height, width) which is passed to a 2D average pooling layer with filter/kernel size (6,4) in the form of (height, width). <br>\n",
    "Code this 2D average pooling layer and print the size of the output image. <br>\n",
    "\n",
    "Hint: You may use function `torch.nn.AvgPool2d(kernel_size=(height,width))`.<br>\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--20-04-39\n",
      "torch.Size([512, 3, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "x = torch.rand(512,3,64,64) # input image  \n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "import torch.nn as nn\n",
    "mod = nn.AvgPool2d(6,4)\n",
    "y=mod(x)\n",
    "\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Given $Y$ a tensor composed of one-hot encoding of class labels, the goal is to smooth $Y$ to decrease over-fitting during training. The smoothing of $Y$ is defined as\n",
    "\n",
    "$$Y^{LS} = (1 - \\alpha )Y + \\alpha/K,$$\n",
    "\n",
    "where $\\alpha$ is a hyper-parameter controling the strength of the regularization and $K$ is the number of classes.\n",
    "\n",
    "Given $Y$ below of the size (batch size, K) and $\\alpha=0.1$, code and print the tensor $Y^{LS}$.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--20-08-57\n",
      "2\n",
      "tensor([[0.0500, 0.9500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
      "        [0.9500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n",
      "        [0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.9500],\n",
      "        [0.0500, 0.0500, 0.0500, 0.0500, 0.9500, 0.0500]])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "Y = torch.Tensor([\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 1, 0] ])\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "K = list(Y.unique().size())[0]\n",
    "print(K)\n",
    "Y_LS = (1-alpha) * Y + alpha/(K)\n",
    "print(Y_LS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Implement the class of the z-score layer defined as\n",
    "\n",
    "$$\n",
    "y = \\frac{x-\\textrm{mean}(x)}{\\textrm{std}(x)} \n",
    "$$\n",
    "\n",
    "where $x$, $y$ are vectors, and $\\textrm{mean},\\textrm{std}$ are the mean and the standard deviation operators. \n",
    "\n",
    "Print the size, mean and standard deviation of the output $y$ of the z-score layer with the input $x$ given below.\n",
    "\n",
    "Hints: You may consider functions *.mean()*, *.std()*. \n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--20-10-56\n",
      "torch.Size([50])\n",
      "tensor(0.5178)\n",
      "tensor(0.3338)\n",
      "tensor([-0.2807, -0.7172,  1.2047,  1.4135, -0.4195, -1.2073, -0.2674, -0.9059,\n",
      "         1.2664, -0.6268, -1.5279,  0.2964, -0.3448, -0.5481,  1.4432, -1.3008,\n",
      "        -1.3738, -0.6391, -0.8100, -0.2163, -0.0106,  0.1841,  0.8716, -1.4745,\n",
      "         0.0916, -0.6566,  0.1166,  0.2424,  1.0026,  1.2817, -1.5183,  1.2654,\n",
      "         0.1154,  1.3780,  1.2799, -1.3417,  1.3366,  1.2977,  0.4972, -1.2427,\n",
      "         0.9781, -0.6320,  1.3891,  1.1325,  0.8392, -0.1866,  0.6140, -1.4300,\n",
      "        -1.5281, -0.3309])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "x = torch.rand(50) # a vector of 50 features \n",
    "\n",
    "# YOUR CODE HERE\n",
    "size = x.size()\n",
    "print(size)\n",
    "mean= x.mean()\n",
    "print(mean)\n",
    "std= x.std()\n",
    "print(std)\n",
    "\n",
    "y = (x - mean)/std \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Implement a convolutional neural network (CNN) for image classification. The image dataset is a subset of MNIST with 1000 training images and 500 test images. \n",
    "\n",
    "__Requirements:__\n",
    "1. Two convolutional layers with 32 filters of size (height=5, width=5) and padding value 2.\n",
    "1. Each convolutional layer is followed by a ReLU activation layer.\n",
    "1. Each ReLU layer is followed by a max pooling layer with filter/kernel size (2,2).\n",
    "1. The rest of the network architecture is arbitrary.\n",
    "1. Only use `train_data` and `train_label` for training and `test_data` for evaluation.\n",
    "1. Train the network for 5 epochs.\n",
    "1. Print the test accuracy for `test_data` and `test_label` after training.\n",
    "1. The test accuracy must be above 80%.\n",
    "\n",
    "Hints: \n",
    "1. Remember that the input tensor of the convolutional layer must be of the form (batch size, number of input channels, height, width). If the input image has only one channel (grayscale images) then you need to create a singleton dimension to have 'number of input channels=1'. You may consider `.unsqueeze(dim=)` to create a singleton dimension, e.g. `x.size()=(14,2)=>x.unsqueeze(dim=1).size()=(14,1,2)`.\n",
    "1. You may consider the Adam optimizer with learning rate 0.01 instead of SGD : `torch.optim.Adam`.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--21-28-58\n",
      "torch.Size([1000, 28, 28]) torch.Size([1000]) torch.Size([500, 28, 28]) torch.Size([500])\n",
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=3136, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "epoch= 0 \t loss= 2.3104617595672607 \t accuracy= 9.899999618530273 percent\n",
      "epoch= 1 \t loss= 8.140653610229492 \t accuracy= 18.899999618530273 percent\n",
      "epoch= 2 \t loss= 2.2792065143585205 \t accuracy= 21.0 percent\n",
      "epoch= 3 \t loss= 6.340246200561523 \t accuracy= 17.399999618530273 percent\n",
      "epoch= 4 \t loss= 2.8006815910339355 \t accuracy= 9.899999618530273 percent\n",
      "Test accuracy=28.399999618530273\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "def get_accuracy(scores, labels):\n",
    "    num_data = scores.size(0)\n",
    "    predicted_labels = scores.argmax(dim=1)\n",
    "    indicator = (predicted_labels == labels)\n",
    "    num_matches = indicator.sum()\n",
    "    return 100*num_matches.float()/num_data  \n",
    "\n",
    "train_data, train_label, test_data, test_label = torch.load('dataset/small_MNIST.pt')\n",
    "print(train_data.size(),train_label.size(),test_data.size(),test_label.size())\n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "import torch.nn as nn\n",
    "# device= torch.device(\"cuda\") \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # CL1:   28 x 28  -->    32 x 28 x 28 \n",
    "        self.conv1 = nn.Conv2d(1,   32,  kernel_size=5,  padding=2 )\n",
    "        # MP1: 32 x 28 x 28 -->    32 x 14 x 14\n",
    "        self.pool1  = nn.MaxPool2d(2,2)\n",
    "        # CL2:   32 x 14 x 14  -->    32 x 14 x 14 \n",
    "        self.conv2 = nn.Conv2d(32,  32,  kernel_size=5,  padding=2 )\n",
    "        # CL2:   32 x 14 x 14  -->    32 x 7 x 7 \n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.linear1 = nn.Linear(1568, 100)\n",
    "        self.linear2 = nn.Linear(100,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 1568)\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "net=CNN()\n",
    "print(net)\n",
    "\n",
    "mean= train_data.mean()\n",
    "std= train_data.std()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr=0.01 \n",
    "for epoch in range(5):\n",
    "    optimizer=torch.optim.Adam( net.parameters() , lr=my_lr )\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    #train_data, train_label, test_data, test_label\n",
    "    optimizer.zero_grad()\n",
    "    train_data = train_data.unsqueeze(dim=1).view(-1,1,28,28)\n",
    "    train_label = train_label\n",
    "\n",
    "    inputs = (train_data - mean)/std \n",
    "    inputs.requires_grad_()\n",
    "    scores=net( inputs ) \n",
    "    loss =  criterion( scores , train_label) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = get_accuracy(scores.detach(), train_label)\n",
    "    running_loss += loss.detach().item()\n",
    "    running_acc += acc.item()\n",
    "    print('epoch=',epoch, '\\t loss=', running_loss , '\\t accuracy=', running_acc ,'percent')\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     optimizer=torch.optim.Adam( net.parameters() , lr=my_lr )\n",
    "#     running_loss=0\n",
    "#     running_acc=0\n",
    "#     shuffled_indices=torch.randperm(1000)\n",
    "#     for count in range(0,1000,bs):\n",
    "#         #train_data, train_label, test_data, test_label\n",
    "#         optimizer.zero_grad()\n",
    "#         indices=shuffled_indices[count:count+bs]\n",
    "#         minibatch_data = train_data[indices].unsqueeze(dim=1)\n",
    "#         minibatch_label = train_label[indices]\n",
    "\n",
    "#         inputs = (minibatch_data - mean)/std \n",
    "#         inputs.requires_grad_()\n",
    "#         scores=net( inputs ) \n",
    "#         loss =  criterion( scores , minibatch_label) \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         acc = get_accuracy(scores.detach(), minibatch_label)\n",
    "#     running_loss += loss.detach().item()\n",
    "#     running_acc += acc.item()\n",
    "#     print('epoch=',epoch, '\\t loss=', loss , '\\t accuracy=', acc ,'percent')\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    data = test_data.unsqueeze(dim=1)\n",
    "    labels = test_label\n",
    "    scores = net(data.view(-1,1,28,28)) \n",
    "    print(\"Test accuracy={}\".format(get_accuracy(scores.detach(),test_label)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Implement the class of LeakyReLU layer defined as \n",
    "\n",
    "$$ \\sigma(x) = \n",
    "\\left\\{\n",
    "\\begin{array}{lll}\n",
    "x &\\textrm{ for } x\\geq 0\\\\\n",
    "\\alpha. x &\\textrm{ for } x< 0\n",
    "\\end{array}\n",
    "\\right., \n",
    "$$\n",
    "\n",
    "where $\\alpha=0.1$ is the slope for the negative $x$ value. \n",
    "\n",
    "Use this non-linear activation in place of the ReLU activation in the CNN architecture of Question 6. Train the new CNN and make sure that the test accuracy is above 85%.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--21-29-05\n",
      "torch.Size([1000, 28, 28]) torch.Size([1000]) torch.Size([500, 28, 28]) torch.Size([500])\n",
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=3136, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "epoch= 0 \t loss= tensor(2.2889, grad_fn=<NllLossBackward>) \t accuracy= tensor(15.) percent\n",
      "epoch= 1 \t loss= tensor(5.2180, grad_fn=<NllLossBackward>) \t accuracy= tensor(24.1000) percent\n",
      "epoch= 2 \t loss= tensor(7.1540, grad_fn=<NllLossBackward>) \t accuracy= tensor(10.) percent\n",
      "epoch= 3 \t loss= tensor(10.5212, grad_fn=<NllLossBackward>) \t accuracy= tensor(9.3000) percent\n",
      "epoch= 4 \t loss= tensor(11.8351, grad_fn=<NllLossBackward>) \t accuracy= tensor(17.4000) percent\n",
      "Test accuracy=10.800000190734863\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "def get_accuracy(scores, labels):\n",
    "    num_data = scores.size(0)\n",
    "    predicted_labels = scores.argmax(dim=1)\n",
    "    indicator = (predicted_labels == labels)\n",
    "    num_matches = indicator.sum()\n",
    "    return 100*num_matches.float()/num_data  \n",
    "\n",
    "train_data, train_label, test_data, test_label = torch.load('dataset/small_MNIST.pt')\n",
    "print(train_data.size(),train_label.size(),test_data.size(),test_label.size())\n",
    "\n",
    "# YOUR CODE STARTS HERE \n",
    "import torch.nn as nn\n",
    "# device= torch.device(\"cuda\") \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # CL1:   28 x 28  -->    32 x 28 x 28 \n",
    "        self.conv1 = nn.Conv2d(1,   32,  kernel_size=5,  padding=2 )\n",
    "        # MP1: 32 x 28 x 28 -->    32 x 14 x 14\n",
    "        self.pool1  = nn.MaxPool2d(2,2)\n",
    "        # CL2:   32 x 14 x 14  -->    32 x 14 x 14 \n",
    "        self.conv2 = nn.Conv2d(32,  32,  kernel_size=5,  padding=2 )\n",
    "        # CL2:   32 x 14 x 14  -->    32 x 7 x 7 \n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.linear1 = nn.Linear(1568, 100)\n",
    "        self.linear2 = nn.Linear(100,10)\n",
    "\n",
    "    def LeakyReLU(self,x):\n",
    "        x[x<0]=0.1*x[x<0]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 1568)\n",
    "        x = self.linear1(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "net=CNN()\n",
    "print(net)\n",
    "\n",
    "mean= train_data.mean()\n",
    "std= train_data.std()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "my_lr=0.01 \n",
    "for epoch in range(5):\n",
    "    optimizer=torch.optim.Adam( net.parameters() , lr=my_lr )\n",
    "    running_loss=0\n",
    "    running_acc=0\n",
    "    #train_data, train_label, test_data, test_label\n",
    "    optimizer.zero_grad()\n",
    "    train_data = train_data.unsqueeze(dim=1).view(-1,1,28,28)\n",
    "    train_label = train_label\n",
    "\n",
    "    inputs = (train_data - mean)/std \n",
    "    inputs.requires_grad_()\n",
    "    scores=net( inputs ) \n",
    "    loss =  criterion( scores , train_label) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = get_accuracy(scores.detach(), train_label)\n",
    "    running_loss += loss.detach().item()\n",
    "    running_acc += acc.item()\n",
    "    print('epoch=',epoch, '\\t loss=', loss , '\\t accuracy=', acc ,'percent')\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    data = test_data.unsqueeze(dim=1)\n",
    "    labels = test_label\n",
    "    scores = net(data.view(-1,1,28,28)) \n",
    "    print(\"Test accuracy={}\".format(get_accuracy(scores.detach(),test_label)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Implement and train a vanilla recurrent neural network (VRNN) for predicting the next world in a sequence. The  dataset is a subset of PTB composed of 20 sub-documents with each training document having 1000 words and each test document having 1000 words as well. \n",
    "\n",
    "__Requirements:__\n",
    "1. Only use `train_data` and `train_label` for training and `test_data` for evaluation.\n",
    "1. The total number of epochs is limited to 5.\n",
    "1. Print the train perplexity at each epoch (remember that perplexity=exp(cross_entropy_loss)).\n",
    "1. The test perplexity must reach a value smaller than 800.\n",
    "\n",
    "Hint: You may choose your own values for the hyper-parameters but you may also consider the value 100 for the hidden dimension, the value 35 for the length of the sequences in the minibatch, and the value 0.5 for the learning rate of SGD.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--21-05-17\n",
      "torch.Size([1000, 20]) torch.Size([1000, 20])\n",
      "three_layer_recurrent_net(\n",
      "  (layer1): Embedding(10000, 100)\n",
      "  (layer2): RNN(100, 100)\n",
      "  (layer3): Linear(in_features=100, out_features=10000, bias=True)\n",
      ")\n",
      "\n",
      "epoch= 0 \t lr= 0.5 \t exp(loss)= 2137.186938639055\n",
      "\n",
      "epoch= 1 \t lr= 0.5 \t exp(loss)= 918.3114983039658\n",
      "\n",
      "epoch= 2 \t lr= 0.5 \t exp(loss)= 750.7961260373391\n",
      "\n",
      "epoch= 3 \t lr= 0.5 \t exp(loss)= 654.9223229363265\n",
      "\n",
      "epoch= 4 \t lr= 0.5 \t exp(loss)= 589.5112976236846\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "train_data, test_data = torch.load('dataset/small_PTB.pt')\n",
    "print(train_data.size(), test_data.size())\n",
    "\n",
    "bs = 20\n",
    "vocab_size = 10000\n",
    "\n",
    "# YOUR CODE STARTS HERE \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import utils\n",
    "def normalize_gradient(net):\n",
    "\n",
    "    grad_norm_sq=0\n",
    "\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "   \n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:    \n",
    "        for p in net.parameters():\n",
    "             p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm\n",
    "class three_layer_recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(three_layer_recurrent_net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = nn.RNN(       hidden_size , hidden_size  )\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "        \n",
    "    def forward(self, word_seq, h_init ):\n",
    "        \n",
    "        g_seq               =   self.layer1( word_seq )  \n",
    "        h_seq , h_final     =   self.layer2( g_seq , h_init )\n",
    "        score_seq           =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  h_final \n",
    "\n",
    "hidden_size=100\n",
    "\n",
    "net = three_layer_recurrent_net( hidden_size )\n",
    "\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 0.5\n",
    "\n",
    "seq_length = 35\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    \n",
    "    # create a new optimizer and give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    \n",
    "    for count in range( 0 , 1000-seq_length ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        h=h.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Implement and train a vanilla recurrent neural network (VRNN) on the small PTB dataset by **explicitly** implementing the VRNN layer (the use of the function nn.RNN() is prohibited) :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_t =& \\textrm{ tanh}(Ah_{t-1}+a+Bx_t+b)\\\\\n",
    "y_t =& \\ C h_{t} +c\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $(A,a)$ are the parameters of the linear transformation (matrix,bias) applied to $h_{t-1}$, $(B,b)$ are the parameters of the linear transformation applied to $x_t$ and $(C,c)$ are the parameters of the linear transformation applied to $h_t$.\n",
    "\n",
    "The  dataset is a subset of PTB composed of 20 sub-documents with each training document having 1000 words and each test document having 1000 words as well. \n",
    "\n",
    "__Requirements:__\n",
    "1. Only use `train_data` and `train_label` for training and `test_data` for evaluation.\n",
    "1. The total number of epochs is limited to 5.\n",
    "1. Print the train perplexity at each epoch.\n",
    "1. The test perplexity must reach a value smaller than 800.\n",
    "\n",
    "Hints: \n",
    "1. Activation function tanh is given by *torch.tanh*\n",
    "1. You may consider creating a list of $h_t$ with `h_seq = []`, add a vector $h_t$ to the list with `h_seq.append(h_t)` and convert the list of vectors into a PyTorch tensor with `h_seq = torch.stack(h_seq, dim=0).squeeze()`.\n",
    "1. You may choose your own values for the hyper-parameters but you may also consider the value 100 for the hidden dimension, the value 35 for the length of the sequences in the minibatch, and the value 0.5 for the learning rate of SGD.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--10-15-11\n",
      "torch.Size([1000, 20]) torch.Size([1000, 20])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "train_data, test_data = torch.load('dataset/small_PTB.pt')\n",
    "print(train_data.size(), test_data.size())\n",
    "\n",
    "bs = 20\n",
    "vocab_size = 10000\n",
    "\n",
    "# YOUR CODE STARTS HERE \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import utils\n",
    "def normalize_gradient(net):\n",
    "\n",
    "    grad_norm_sq=0\n",
    "\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "   \n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:    \n",
    "        for p in net.parameters():\n",
    "             p.grad.data.div_(grad_norm)\n",
    "\n",
    "    return grad_norm\n",
    "class three_layer_recurrent_net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size):\n",
    "        super(three_layer_recurrent_net, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = nn.RNN(       hidden_size , hidden_size  )\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "\n",
    "    def activate(self,x):\n",
    "        \n",
    "        \n",
    "    def forward(self, word_seq, h_init ):\n",
    "        \n",
    "        g_seq               =   self.layer1( word_seq )  \n",
    "        h_seq , h_final     =   self.layer2( g_seq , h_init )\n",
    "        score_seq           =   self.layer3( h_seq )\n",
    "        \n",
    "        return score_seq,  h_final \n",
    "\n",
    "hidden_size=100\n",
    "\n",
    "net = three_layer_recurrent_net( hidden_size )\n",
    "\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "my_lr = 0.5\n",
    "\n",
    "seq_length = 35\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    # keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch\n",
    "    \n",
    "    # create a new optimizer and give the current learning rate.   \n",
    "    optimizer=torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "        \n",
    "    # set the running quantities to zero at the beginning of the epoch\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "       \n",
    "    # set the initial h to be the zero vector\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    \n",
    "    for count in range( 0 , 1000-seq_length ,  seq_length):\n",
    "             \n",
    "        # Set the gradients to zeros\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create a minibatch\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]        \n",
    "        \n",
    "        # Detach to prevent from backpropagating all the way to the beginning\n",
    "        # Then tell Pytorch to start tracking all operations that will be done on h and c\n",
    "        h=h.detach()\n",
    "        h=h.requires_grad_()\n",
    "                       \n",
    "        # forward the minibatch through the net        \n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        \n",
    "        # reshape the scores and labels to huge batch of size bs*seq_length\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)  \n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )       \n",
    "        \n",
    "        # Compute the average of the losses of the data points in this huge batch\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        \n",
    "        # backward pass to compute dL/dR, dL/dV and dL/dW\n",
    "        loss.backward()\n",
    "\n",
    "        # do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...\n",
    "        normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "        # update the running loss  \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # compute stats for the full training set\n",
    "    total_loss = running_loss/num_batches\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Implement and train a gated recurrent unit network (GRU) on the small PTB dataset by **explicitly** implementing the GRU layer (the use of the function nn.GRU() is prohibited) :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "r_t =& \\textrm{ sigmoid}(A x_t + a + B h_{t-1} + b)\\\\\n",
    "z_t =& \\textrm{ sigmoid}(C x_t + c + D h_{t-1} + d)\\\\\n",
    "n_t =& \\textrm{ tanh} (E x_t + e + r_t \\odot (F h_{t-1}+f))\\\\\n",
    "h_t =& (1-z_t) \\odot n_{t} + z_t \\odot h_{t-1}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $(A,a), (B,b), (C,c), (D,d), (E,e), (F,f)$ are the parameters (matrix,bias) of all linear transformations and \n",
    "$\\odot$ is the element-wise product operator or Hadamard product. \n",
    "\n",
    "The  dataset is a subset of PTB composed of 20 sub-documents with each training document having 1000 words and each test document having 1000 words as well. \n",
    "\n",
    "__Requirements:__\n",
    "1. Only use `train_data` and `train_label` for training and `test_data` for evaluation.\n",
    "1. The total number of epochs is limited to 5.\n",
    "1. Print the train perplexity at each epoch.\n",
    "1. The test perplexity must reach a value smaller than 800.\n",
    "\n",
    "Hints: \n",
    "1. Activation function sigmoid is given by *torch.sigmoid*.\n",
    "1. The Hadamard product $\\odot$ is given by `*`.\n",
    "\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 21-11-02--10-15-12\n",
      "torch.Size([1000, 20]) torch.Size([1000, 20])\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import torch\n",
    "import datetime\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "train_data, test_data = torch.load('dataset/small_PTB.pt')\n",
    "print(train_data.size(), test_data.size())\n",
    "\n",
    "bs = 20\n",
    "vocab_size = 10000\n",
    "\n",
    "# YOUR CODE STARTS HERE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNd30TjgREYWyAkV0lT4iX6",
   "name": "CE_7454_coding_test_solution_Vijay.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "188299b107256fe873ab79b6089594fb9be85d55d578db9419dca9181f4278ba"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('deeplearn_course': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
